{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import pipeline as pipe\n",
    "lemma = pipe.getTxtLemmatization([\"train_pos.txt\", \"train_neg.txt\"],\n",
    "                        stopwords = False,\n",
    "                        replace = True,\n",
    "                        replace_stanford=False,\n",
    "                        lemmatize = True,\n",
    "                        outputfiles = [\"redoing1_pos.txt\",\"redoing1_neg.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import tokenizer as tok\n",
    "voc = tok.build_vocab(frequency_treshold=500,\n",
    "                file_name=\"redoing_vocab1.pkl\",\n",
    "                use_base_vocabulary=False,\n",
    "                base_vocabulary_name=\"stanford_vocab.pkl\",\n",
    "                input_files= [\"redoing1_pos.txt\", \"redoing1_neg.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import cooc\n",
    "\n",
    "cooc_mat = cooc.build_cooc(\"redoing_vocab1.pkl\",\n",
    "               window_size=None,\n",
    "               weighting=\"None\",\n",
    "               output_name=\"redoing_cooc1.pkl\",\n",
    "               input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " from embedding import pipeline as pipe\n",
    "import pickle\n",
    "from embedding.embedding_base import EmbeddingBase\n",
    "import pandas as pd\n",
    "from embedding import sentence_embedding\n",
    "import numpy as np\n",
    "from classifier import Adaboost_classi as classy\n",
    "import embedding.pipeline as emb     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "620"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb.embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n"
     ]
    }
   ],
   "source": [
    "emb1 = emb.get_glove_embedding(vocabulary_file=\"redoing_vocab1.pkl\",\n",
    "                        cooc_file=\"redoing_cooc1.pkl\",\n",
    "                        load_from_file=False,\n",
    "                        file_name = \"redoing_emb1.npz\",\n",
    "                        load_Stanford=False,\n",
    "                        train=True,\n",
    "                        save=True,\n",
    "                        train_epochs=10,\n",
    "                        train_eta=1e-3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(\"redoing_vocab1.pkl\", \"rb\") as f:\n",
    "    voc = pickle.load(f)\n",
    "with open(\"redoing_cooc1.pkl\", \"rb\") as f:\n",
    "    cooc = pickle.load(f)\n",
    "    \n",
    "emb = EmbeddingBase(embedding_name = \"redoing_emb1.npz\",\n",
    "                 embedding_dimension = 200,\n",
    "                 vocabulary = voc,\n",
    "                 cooc = cooc,\n",
    "                 load = True)\n",
    "\n",
    "mat1 = pipe.build_training_matrix(label = True,\n",
    "                          embedding = emb,\n",
    "                          input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"],\n",
    "                          label_values=None,\n",
    "                          input_entries=200000,\n",
    "                          sentence_dimesion = 200,\n",
    "                          output_location = \"D:\\\\embedding_matrices\\\\redoing1_training_1\")\n",
    "\n",
    "mat2 = pipe.build_training_matrix(label = True,\n",
    "                          embedding = emb,\n",
    "                          input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"],\n",
    "                          label_values=None,\n",
    "                          input_entries=200000,\n",
    "                          aggregation_fun=sentence_embedding.no_embeddings,\n",
    "                          sentence_dimesion = 100,\n",
    "                          output_location = \"D:\\\\embedding_matrices\\\\redoing1_training_0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model.\n",
      "Training model\n",
      "[0.639475 0.66585  0.668225 0.665475 0.658825]\n",
      "Building model.\n",
      "Training model\n",
      "[0.67915  0.675725 0.664975 0.6695   0.66375 ]\n"
     ]
    }
   ],
   "source": [
    "ada1 = classy.Adaboost_classi(embedding_dimension=200)\n",
    "arr0 = np.load(\"D://embedding_matrices//redoing1_training_0.npz\")\n",
    "arr1 = np.load(\"D://embedding_matrices//redoing1_training_1.npz\")\n",
    "arr1 = arr1['arr_0']\n",
    "arr0 = arr0['arr_0']\n",
    "x = arr0[:,:-1]\n",
    "y = arr0[:,-1]\n",
    "ada1.build()\n",
    "ada1.train(x,y)\n",
    "x = arr1[:,:-1]\n",
    "y = arr1[:,-1]\n",
    "ada1.build()\n",
    "ada1.train(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import pipeline as pipe\n",
    "from preprocessing import tokenizer as tok\n",
    "from preprocessing import cooc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(emb.embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(voc)\n",
    "\n",
    "#20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-726e1618d426>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'arr0' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(arr0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocs = [20, 30 ,50, 100, 200,1000,2000,4000,6000,8000,10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT NUMBER 1\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 20\n",
      "Building model.\n",
      "Training model\n",
      "[0.680325 0.658475 0.654125 0.648725 0.647525]\n",
      "Building model.\n",
      "Training model\n",
      "[0.7461   0.7474   0.7326   0.73475  0.730725]\n",
      "EXPERIMENT NUMBER 2\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 30\n",
      "Building model.\n",
      "Training model\n",
      "[0.6774   0.661475 0.6538   0.650925 0.647925]\n",
      "Building model.\n",
      "Training model\n",
      "[0.75065  0.745725 0.736025 0.737075 0.727625]\n",
      "EXPERIMENT NUMBER 3\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 50\n",
      "Building model.\n",
      "Training model\n",
      "[0.673525 0.663    0.657475 0.6451   0.646475]\n",
      "Building model.\n",
      "Training model\n",
      "[0.743825 0.7433   0.73315  0.73565  0.726925]\n",
      "EXPERIMENT NUMBER 4\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 100\n",
      "Building model.\n",
      "Training model\n",
      "[0.64295  0.6667   0.661025 0.659025 0.64955 ]\n",
      "Building model.\n",
      "Training model\n",
      "[0.743575 0.740975 0.7302   0.73245  0.7269  ]\n",
      "EXPERIMENT NUMBER 5\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 200\n",
      "Building model.\n",
      "Training model\n",
      "[0.641875 0.665125 0.66155  0.661275 0.6577  ]\n",
      "Building model.\n",
      "Training model\n",
      "[0.73855  0.73525  0.731025 0.729125 0.728825]\n",
      "EXPERIMENT NUMBER 6\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 1000\n",
      "Building model.\n",
      "Training model\n",
      "[0.646675 0.67385  0.672125 0.667775 0.65945 ]\n",
      "Building model.\n",
      "Training model\n",
      "[0.73115  0.7164   0.718725 0.718575 0.713975]\n",
      "EXPERIMENT NUMBER 7\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 2000\n",
      "Building model.\n",
      "Training model\n",
      "[0.658475 0.683375 0.67585  0.68075  0.66805 ]\n",
      "Building model.\n",
      "Training model\n",
      "[0.73285  0.71675  0.713975 0.715275 0.709   ]\n",
      "EXPERIMENT NUMBER 8\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 4000\n",
      "Building model.\n",
      "Training model\n",
      "[0.66315  0.6851   0.686425 0.6852   0.679875]\n",
      "Building model.\n",
      "Training model\n",
      "[0.7213   0.70065  0.705275 0.70425  0.697275]\n",
      "EXPERIMENT NUMBER 9\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 6000\n",
      "Building model.\n",
      "Training model\n",
      "[0.6912   0.686    0.686025 0.6857   0.674375]\n",
      "Building model.\n",
      "Training model\n",
      "[0.708025 0.7015   0.702825 0.703325 0.689925]\n",
      "EXPERIMENT NUMBER 10\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 8000\n",
      "Building model.\n",
      "Training model\n",
      "[0.695125 0.6836   0.6841   0.683425 0.674375]\n",
      "Building model.\n",
      "Training model\n",
      "[0.6706   0.696275 0.697625 0.684675 0.68385 ]\n",
      "EXPERIMENT NUMBER 11\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 10000\n",
      "Building model.\n",
      "Training model\n",
      "[0.700875 0.68765  0.684225 0.684875 0.676075]\n",
      "Building model.\n",
      "Training model\n",
      "[0.669075 0.7042   0.697925 0.6951   0.68755 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0,11):\n",
    "\n",
    "    from preprocessing import pipeline as pipe\n",
    "    print(\"EXPERIMENT NUMBER\", i+1)\n",
    "    lemma = pipe.getTxtLemmatization([\"train_pos.txt\", \"train_neg.txt\"],\n",
    "                            stopwords = False,\n",
    "                            replace = True,\n",
    "                            replace_stanford=False,\n",
    "                            lemmatize = True,\n",
    "                            outputfiles = [\"redoing1_pos.txt\",\"redoing1_neg.txt\"])\n",
    "    \n",
    "    voc = tok.build_vocab(frequency_treshold=vocs[i],\n",
    "                    file_name=\"redoing_vocab1.pkl\",\n",
    "                    use_base_vocabulary=False,\n",
    "                    base_vocabulary_name=\"stanford_vocab.pkl\",\n",
    "                    input_files= [\"redoing1_pos.txt\", \"redoing1_neg.txt\"])\n",
    "    from embedding import pipeline as pipe\n",
    "    from preprocessing import cooc\n",
    "    from embedding import pipeline as pipe\n",
    "    import pickle\n",
    "    from embedding.embedding_base import EmbeddingBase\n",
    "    import pandas as pd\n",
    "    from embedding import sentence_embedding\n",
    "    import numpy as np\n",
    "    from classifier import Adaboost_classi as classy\n",
    "    import embedding.pipeline as emb     \n",
    "    cooc_mat = cooc.build_cooc(\"redoing_vocab1.pkl\",\n",
    "                   window_size=None,\n",
    "                   weighting=\"None\",\n",
    "                   output_name=\"redoing_cooc1.pkl\",\n",
    "                   input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"])\n",
    "    emb1 = emb.get_glove_embedding(vocabulary_file=\"redoing_vocab1.pkl\",\n",
    "                            cooc_file=\"redoing_cooc1.pkl\",\n",
    "                            load_from_file=False,\n",
    "                            file_name = \"redoing_emb1.npz\",\n",
    "                            load_Stanford=True,\n",
    "                            train=True,\n",
    "                            save=True,\n",
    "                            train_epochs=10,\n",
    "                            train_eta=1e-3)   \n",
    "\n",
    "\n",
    "    with open(\"redoing_vocab1.pkl\", \"rb\") as f:\n",
    "        voc = pickle.load(f)\n",
    "        \n",
    "\n",
    "    with open(\"redoing_cooc1.pkl\", \"rb\") as f:\n",
    "        cooc = pickle.load(f)\n",
    "\n",
    "    emb = EmbeddingBase(embedding_name = \"redoing_emb1.npz\",\n",
    "                     embedding_dimension = 200,\n",
    "                     vocabulary = voc,\n",
    "                     cooc = cooc,\n",
    "                     load = True)\n",
    "\n",
    "    mat1 = pipe.build_training_matrix(label = True,\n",
    "                              embedding = emb,\n",
    "                              input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"],\n",
    "                              label_values=None,\n",
    "                              input_entries=200000,\n",
    "                              sentence_dimesion = 200,\n",
    "                              output_location = \"D:\\\\embedding_matrices\\\\redoing1_training_1\")\n",
    "\n",
    "    mat2 = pipe.build_training_matrix(label = True,\n",
    "                              embedding = emb,\n",
    "                              input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"],\n",
    "                              label_values=None,\n",
    "                              input_entries=200000,\n",
    "                              aggregation_fun=sentence_embedding.no_embeddings,\n",
    "                              sentence_dimesion = 100,\n",
    "                              output_location = \"D:\\\\embedding_matrices\\\\redoing1_training_0\")\n",
    "    ada1 = classy.Adaboost_classi(embedding_dimension=200)\n",
    "    print(\"Vocabulary:\", vocs[i])\n",
    "    arr0 = np.load(\"D://embedding_matrices//redoing1_training_0.npz\")\n",
    "    arr1 = np.load(\"D://embedding_matrices//redoing1_training_1.npz\")\n",
    "    arr1 = arr1['arr_0']\n",
    "    arr0 = arr0['arr_0']\n",
    "    x = arr0[:,:-1]\n",
    "    y = arr0[:,-1]\n",
    "    ada1.build()\n",
    "    ada1.train(x,y)\n",
    "    x = arr1[:,:-1]\n",
    "    y = arr1[:,-1]\n",
    "    ada1.build()\n",
    "    ada1.train(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocs = [12000, 14000,20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT NUMBER 1\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 12000\n",
      "Building model.\n",
      "Training model\n",
      "[0.65825  0.67965  0.6826   0.679425 0.67635 ]\n",
      "Building model.\n",
      "Training model\n",
      "[0.706    0.693525 0.69675  0.682525 0.6801  ]\n",
      "EXPERIMENT NUMBER 2\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 14000\n",
      "Building model.\n",
      "Training model\n",
      "[0.658025 0.678975 0.680325 0.676975 0.674075]\n",
      "Building model.\n",
      "Training model\n",
      "[0.702    0.690625 0.6952   0.682875 0.6808  ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0,2):\n",
    "\n",
    "    from preprocessing import pipeline as pipe\n",
    "    print(\"EXPERIMENT NUMBER\", i+1)\n",
    "    lemma = pipe.getTxtLemmatization([\"train_pos.txt\", \"train_neg.txt\"],\n",
    "                            stopwords = False,\n",
    "                            replace = True,\n",
    "                            replace_stanford=False,\n",
    "                            lemmatize = True,\n",
    "                            outputfiles = [\"redoing1_pos.txt\",\"redoing1_neg.txt\"])\n",
    "    \n",
    "    voc = tok.build_vocab(frequency_treshold=vocs[i],\n",
    "                    file_name=\"redoing_vocab1.pkl\",\n",
    "                    use_base_vocabulary=False,\n",
    "                    base_vocabulary_name=\"stanford_vocab.pkl\",\n",
    "                    input_files= [\"redoing1_pos.txt\", \"redoing1_neg.txt\"])\n",
    "    from embedding import pipeline as pipe\n",
    "    from preprocessing import cooc\n",
    "    from embedding import pipeline as pipe\n",
    "    import pickle\n",
    "    from embedding.embedding_base import EmbeddingBase\n",
    "    import pandas as pd\n",
    "    from embedding import sentence_embedding\n",
    "    import numpy as np\n",
    "    from classifier import Adaboost_classi as classy\n",
    "    import embedding.pipeline as emb     \n",
    "    cooc_mat = cooc.build_cooc(\"redoing_vocab1.pkl\",\n",
    "                   window_size=None,\n",
    "                   weighting=\"None\",\n",
    "                   output_name=\"redoing_cooc1.pkl\",\n",
    "                   input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"])\n",
    "    emb1 = emb.get_glove_embedding(vocabulary_file=\"redoing_vocab1.pkl\",\n",
    "                            cooc_file=\"redoing_cooc1.pkl\",\n",
    "                            load_from_file=False,\n",
    "                            file_name = \"redoing_emb1.npz\",\n",
    "                            load_Stanford=True,\n",
    "                            train=True,\n",
    "                            save=True,\n",
    "                            train_epochs=10,\n",
    "                            train_eta=1e-3)   \n",
    "\n",
    "\n",
    "    with open(\"redoing_vocab1.pkl\", \"rb\") as f:\n",
    "        voc = pickle.load(f)\n",
    "    with open(\"redoing_cooc1.pkl\", \"rb\") as f:\n",
    "        cooc = pickle.load(f)\n",
    "\n",
    "    emb = EmbeddingBase(embedding_name = \"redoing_emb1.npz\",\n",
    "                     embedding_dimension = 200,\n",
    "                     vocabulary = voc,\n",
    "                     cooc = cooc,\n",
    "                     load = True)\n",
    "\n",
    "    mat1 = pipe.build_training_matrix(label = True,\n",
    "                              embedding = emb,\n",
    "                              input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"],\n",
    "                              label_values=None,\n",
    "                              input_entries=200000,\n",
    "                              sentence_dimesion = 200,\n",
    "                              output_location = \"D:\\\\embedding_matrices\\\\redoing1_training_1\")\n",
    "\n",
    "    mat2 = pipe.build_training_matrix(label = True,\n",
    "                              embedding = emb,\n",
    "                              input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"],\n",
    "                              label_values=None,\n",
    "                              input_entries=200000,\n",
    "                              aggregation_fun=sentence_embedding.no_embeddings,\n",
    "                              sentence_dimesion = 100,\n",
    "                              output_location = \"D:\\\\embedding_matrices\\\\redoing1_training_0\")\n",
    "    ada1 = classy.Adaboost_classi(embedding_dimension=200)\n",
    "    print(\"Vocabulary:\", vocs[i])\n",
    "    arr0 = np.load(\"D://embedding_matrices//redoing1_training_0.npz\")\n",
    "    arr1 = np.load(\"D://embedding_matrices//redoing1_training_1.npz\")\n",
    "    arr1 = arr1['arr_0']\n",
    "    arr0 = arr0['arr_0']\n",
    "    x = arr0[:,:-1]\n",
    "    y = arr0[:,-1]\n",
    "    ada1.build()\n",
    "    ada1.train(x,y)\n",
    "    x = arr1[:,:-1]\n",
    "    y = arr1[:,-1]\n",
    "    ada1.build()\n",
    "    ada1.train(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT NUMBER 1\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 18000\n",
      "Building model.\n",
      "Training model\n",
      "[0.6841   0.677975 0.678525 0.67475  0.6696  ]\n",
      "Building model.\n",
      "Training model\n",
      "[0.69825  0.6959   0.697625 0.693425 0.680575]\n",
      "EXPERIMENT NUMBER 2\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 20000\n",
      "Building model.\n",
      "Training model\n",
      "[0.6839   0.6758   0.68015  0.674575 0.6722  ]\n",
      "Building model.\n",
      "Training model\n",
      "[0.704675 0.69135  0.697975 0.696775 0.68805 ]\n",
      "EXPERIMENT NUMBER 3\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 30000\n",
      "Building model.\n",
      "Training model\n",
      "[0.682925 0.6849   0.677925 0.67955  0.673525]\n",
      "Building model.\n",
      "Training model\n",
      "[0.7004   0.687325 0.69315  0.69035  0.67715 ]\n",
      "EXPERIMENT NUMBER 4\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 40000\n",
      "Building model.\n",
      "Training model\n",
      "[0.681875 0.680175 0.67905  0.679325 0.6738  ]\n",
      "Building model.\n",
      "Training model\n",
      "[0.705675 0.6887   0.693575 0.692525 0.683425]\n",
      "EXPERIMENT NUMBER 5\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-0acb6e54cbff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                             outputfiles = [\"redoing1_pos.txt\",\"redoing1_neg.txt\"])\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     voc = tok.build_vocab(frequency_treshold=vocs[i],\n\u001b[0m\u001b[0;32m     14\u001b[0m                     \u001b[0mfile_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"redoing_vocab1.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[0muse_base_vocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "vocs = [18000,20000,30000,40000]\n",
    "for i in range(0,5):\n",
    "\n",
    "    from preprocessing import pipeline as pipe\n",
    "    print(\"EXPERIMENT NUMBER\", i+1)\n",
    "    lemma = pipe.getTxtLemmatization([\"train_pos.txt\", \"train_neg.txt\"],\n",
    "                            stopwords = False,\n",
    "                            replace = True,\n",
    "                            replace_stanford=False,\n",
    "                            lemmatize = True,\n",
    "                            outputfiles = [\"redoing1_pos.txt\",\"redoing1_neg.txt\"])\n",
    "    \n",
    "    voc = tok.build_vocab(frequency_treshold=vocs[i],\n",
    "                    file_name=\"redoing_vocab1.pkl\",\n",
    "                    use_base_vocabulary=False,\n",
    "                    base_vocabulary_name=\"stanford_vocab.pkl\",\n",
    "                    input_files= [\"redoing1_pos.txt\", \"redoing1_neg.txt\"])\n",
    "    from embedding import pipeline as pipe\n",
    "    from preprocessing import cooc\n",
    "    from embedding import pipeline as pipe\n",
    "    import pickle\n",
    "    from embedding.embedding_base import EmbeddingBase\n",
    "    import pandas as pd\n",
    "    from embedding import sentence_embedding\n",
    "    import numpy as np\n",
    "    from classifier import Adaboost_classi as classy\n",
    "    import embedding.pipeline as emb     \n",
    "    cooc_mat = cooc.build_cooc(\"redoing_vocab1.pkl\",\n",
    "                   window_size=None,\n",
    "                   weighting=\"None\",\n",
    "                   output_name=\"redoing_cooc1.pkl\",\n",
    "                   input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"])\n",
    "    emb1 = emb.get_glove_embedding(vocabulary_file=\"redoing_vocab1.pkl\",\n",
    "                            cooc_file=\"redoing_cooc1.pkl\",\n",
    "                            load_from_file=False,\n",
    "                            file_name = \"redoing_emb1.npz\",\n",
    "                            load_Stanford=True,\n",
    "                            train=True,\n",
    "                            save=True,\n",
    "                            train_epochs=10,\n",
    "                            train_eta=1e-3)   \n",
    "\n",
    "\n",
    "    with open(\"redoing_vocab1.pkl\", \"rb\") as f:\n",
    "        voc = pickle.load(f)\n",
    "    with open(\"redoing_cooc1.pkl\", \"rb\") as f:\n",
    "        cooc = pickle.load(f)\n",
    "\n",
    "    emb = EmbeddingBase(embedding_name = \"redoing_emb1.npz\",\n",
    "                     embedding_dimension = 200,\n",
    "                     vocabulary = voc,\n",
    "                     cooc = cooc,\n",
    "                     load = True)\n",
    "\n",
    "    mat1 = pipe.build_training_matrix(label = True,\n",
    "                              embedding = emb,\n",
    "                              input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"],\n",
    "                              label_values=None,\n",
    "                              input_entries=200000,\n",
    "                              sentence_dimesion = 200,\n",
    "                              output_location = \"D:\\\\embedding_matrices\\\\redoing1_training_1\")\n",
    "\n",
    "    mat2 = pipe.build_training_matrix(label = True,\n",
    "                              embedding = emb,\n",
    "                              input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"],\n",
    "                              label_values=None,\n",
    "                              input_entries=200000,\n",
    "                              aggregation_fun=sentence_embedding.no_embeddings,\n",
    "                              sentence_dimesion = 100,\n",
    "                              output_location = \"D:\\\\embedding_matrices\\\\redoing1_training_0\")\n",
    "    ada1 = classy.Adaboost_classi(embedding_dimension=200)\n",
    "    print(\"Vocabulary:\", vocs[i])\n",
    "    arr0 = np.load(\"D://embedding_matrices//redoing1_training_0.npz\")\n",
    "    arr1 = np.load(\"D://embedding_matrices//redoing1_training_1.npz\")\n",
    "    arr1 = arr1['arr_0']\n",
    "    arr0 = arr0['arr_0']\n",
    "    x = arr0[:,:-1]\n",
    "    y = arr0[:,-1]\n",
    "    ada1.build()\n",
    "    ada1.train(x,y)\n",
    "    x = arr1[:,:-1]\n",
    "    y = arr1[:,-1]\n",
    "    ada1.build()\n",
    "    ada1.train(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT NUMBER 1\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 50000\n",
      "Building model.\n",
      "Training model\n",
      "[0.656825 0.64645  0.6516   0.635675 0.639675]\n",
      "Building model.\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 134, in fit\n",
      "    random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 193, in fit\n",
      "    return_inverse=True)\n",
      "  File \"<__array_function__ internals>\", line 6, in unique\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 261, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 320, in _unique1d\n",
      "    aux = ar[perm]\n",
      "MemoryError: Unable to allocate 1.22 MiB for an array with shape (160000,) and data type float64\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 134, in fit\n",
      "    random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 158, in fit\n",
      "    check_y_params))\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\base.py\", line 429, in _validate_data\n",
      "    X = check_array(X, **check_X_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 599, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 83, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "MemoryError: Unable to allocate 122. MiB for an array with shape (160000, 200) and data type float32\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 134, in fit\n",
      "    random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 158, in fit\n",
      "    check_y_params))\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\base.py\", line 429, in _validate_data\n",
      "    X = check_array(X, **check_X_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 599, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 83, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "MemoryError: Unable to allocate 122. MiB for an array with shape (160000, 200) and data type float32\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 134, in fit\n",
      "    random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 193, in fit\n",
      "    return_inverse=True)\n",
      "  File \"<__array_function__ internals>\", line 6, in unique\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 261, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 314, in _unique1d\n",
      "    ar = np.asanyarray(ar).flatten()\n",
      "MemoryError: Unable to allocate 1.22 MiB for an array with shape (160000,) and data type float64\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 134, in fit\n",
      "    random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 193, in fit\n",
      "    return_inverse=True)\n",
      "  File \"<__array_function__ internals>\", line 6, in unique\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 261, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 314, in _unique1d\n",
      "    ar = np.asanyarray(ar).flatten()\n",
      "MemoryError: Unable to allocate 1.22 MiB for an array with shape (160000,) and data type float64\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan]\n",
      "EXPERIMENT NUMBER 2\n",
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n",
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n",
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n",
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n",
      "Vocabulary: 60000\n",
      "Building model.\n",
      "Training model\n",
      "[0.660075 0.6459   0.650625 0.644    0.63465 ]\n",
      "Building model.\n",
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 134, in fit\n",
      "    random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 158, in fit\n",
      "    check_y_params))\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\base.py\", line 429, in _validate_data\n",
      "    X = check_array(X, **check_X_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 599, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 83, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "MemoryError: Unable to allocate 122. MiB for an array with shape (160000, 200) and data type float32\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 134, in fit\n",
      "    random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 181, in fit\n",
      "    check_classification_targets(y)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\utils\\multiclass.py\", line 169, in check_classification_targets\n",
      "    y_type = type_of_target(y)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\utils\\multiclass.py\", line 290, in type_of_target\n",
      "    if (len(np.unique(y)) > 2) or (y.ndim >= 2 and len(y[0]) > 1):\n",
      "  File \"<__array_function__ internals>\", line 6, in unique\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 261, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 314, in _unique1d\n",
      "    ar = np.asanyarray(ar).flatten()\n",
      "MemoryError: Unable to allocate 1.22 MiB for an array with shape (160000,) and data type float64\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 134, in fit\n",
      "    random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 193, in fit\n",
      "    return_inverse=True)\n",
      "  File \"<__array_function__ internals>\", line 6, in unique\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 261, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 319, in _unique1d\n",
      "    perm = ar.argsort(kind='mergesort' if return_index else 'quicksort')\n",
      "MemoryError: Unable to allocate 1.22 MiB for an array with shape (160000,) and data type int64\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 134, in fit\n",
      "    random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 158, in fit\n",
      "    check_y_params))\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\base.py\", line 429, in _validate_data\n",
      "    X = check_array(X, **check_X_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 599, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 83, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "MemoryError: Unable to allocate 122. MiB for an array with shape (160000, 200) and data type float32\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 443, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 134, in fit\n",
      "    random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 503, in _boost\n",
      "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 513, in _boost_real\n",
      "    estimator.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 158, in fit\n",
      "    check_y_params))\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\base.py\", line 429, in _validate_data\n",
      "    X = check_array(X, **check_X_params)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 599, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\franc\\Anaconda3\\envs\\brand_new\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 83, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "MemoryError: Unable to allocate 122. MiB for an array with shape (160000, 200) and data type float32\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan]\n",
      "EXPERIMENT NUMBER 3\n",
      "Starting replacement\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-839542c13e6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                             \u001b[0mreplace_stanford\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                             \u001b[0mlemmatize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                             outputfiles = [\"redoing1_pos.txt\",\"redoing1_neg.txt\"])\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     voc = tok.build_vocab(frequency_treshold=vocs[i],\n",
      "\u001b[1;32m~\\Desktop\\Econobox-SA\\Econobox-SA-master\\preprocessing\\pipeline.py\u001b[0m in \u001b[0;36mgetTxtLemmatization\u001b[1;34m(input_files, stopwords, replace, replace_stanford, lemmatize, outputfiles)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mfile_replaced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemm_rep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             with open(\"replaced_\"+input_file, \"w\",\n\u001b[0;32m    108\u001b[0m                       encoding='utf8') as f:\n",
      "\u001b[1;32m~\\Desktop\\Econobox-SA\\Econobox-SA-master\\preprocessing\\lemmatizer.py\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\brand_new\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocs = [50000,60000,80000,90000]\n",
    "for i in range(0,4):\n",
    "\n",
    "    from preprocessing import pipeline as pipe\n",
    "    print(\"EXPERIMENT NUMBER\", i+1)\n",
    "    lemma = pipe.getTxtLemmatization([\"train_pos.txt\", \"train_neg.txt\"],\n",
    "                            stopwords = False,\n",
    "                            replace = True,\n",
    "                            replace_stanford=False,\n",
    "                            lemmatize = True,\n",
    "                            outputfiles = [\"redoing1_pos.txt\",\"redoing1_neg.txt\"])\n",
    "    \n",
    "    voc = tok.build_vocab(frequency_treshold=vocs[i],\n",
    "                    file_name=\"redoing_vocab1.pkl\",\n",
    "                    use_base_vocabulary=False,\n",
    "                    base_vocabulary_name=\"stanford_vocab.pkl\",\n",
    "                    input_files= [\"redoing1_pos.txt\", \"redoing1_neg.txt\"])\n",
    "    from embedding import pipeline as pipe\n",
    "    from preprocessing import cooc\n",
    "    from embedding import pipeline as pipe\n",
    "    import pickle\n",
    "    from embedding.embedding_base import EmbeddingBase\n",
    "    import pandas as pd\n",
    "    from embedding import sentence_embedding\n",
    "    import numpy as np\n",
    "    from classifier import Adaboost_classi as classy\n",
    "    import embedding.pipeline as emb     \n",
    "    cooc_mat = cooc.build_cooc(\"redoing_vocab1.pkl\",\n",
    "                   window_size=None,\n",
    "                   weighting=\"None\",\n",
    "                   output_name=\"redoing_cooc1.pkl\",\n",
    "                   input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"])\n",
    "    emb1 = emb.get_glove_embedding(vocabulary_file=\"redoing_vocab1.pkl\",\n",
    "                            cooc_file=\"redoing_cooc1.pkl\",\n",
    "                            load_from_file=False,\n",
    "                            file_name = \"redoing_emb1.npz\",\n",
    "                            load_Stanford=True,\n",
    "                            train=True,\n",
    "                            save=True,\n",
    "                            train_epochs=10,\n",
    "                            train_eta=1e-3)   \n",
    "\n",
    "\n",
    "    with open(\"redoing_vocab1.pkl\", \"rb\") as f:\n",
    "        voc = pickle.load(f)\n",
    "    with open(\"redoing_cooc1.pkl\", \"rb\") as f:\n",
    "        cooc = pickle.load(f)\n",
    "\n",
    "    emb = EmbeddingBase(embedding_name = \"redoing_emb1.npz\",\n",
    "                     embedding_dimension = 200,\n",
    "                     vocabulary = voc,\n",
    "                     cooc = cooc,\n",
    "                     load = True)\n",
    "\n",
    "    mat1 = pipe.build_training_matrix(label = True,\n",
    "                              embedding = emb,\n",
    "                              input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"],\n",
    "                              label_values=None,\n",
    "                              input_entries=200000,\n",
    "                              sentence_dimesion = 200,\n",
    "                              output_location = \"D:\\\\embedding_matrices\\\\redoing1_training_1\")\n",
    "\n",
    "    mat2 = pipe.build_training_matrix(label = True,\n",
    "                              embedding = emb,\n",
    "                              input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"],\n",
    "                              label_values=None,\n",
    "                              input_entries=200000,\n",
    "                              aggregation_fun=sentence_embedding.no_embeddings,\n",
    "                              sentence_dimesion = 100,\n",
    "                              output_location = \"D:\\\\embedding_matrices\\\\redoing1_training_0\")\n",
    "    ada1 = classy.Adaboost_classi(embedding_dimension=200)\n",
    "    print(\"Vocabulary:\", vocs[i])\n",
    "    arr0 = np.load(\"D://embedding_matrices//redoing1_training_0.npz\")\n",
    "    arr1 = np.load(\"D://embedding_matrices//redoing1_training_1.npz\")\n",
    "    arr1 = arr1['arr_0']\n",
    "    arr0 = arr0['arr_0']\n",
    "    x = arr0[:,:-1]\n",
    "    y = arr0[:,-1]\n",
    "    ada1.build()\n",
    "    ada1.train(x,y)\n",
    "    x = arr1[:,:-1]\n",
    "    y = arr1[:,-1]\n",
    "    ada1.build()\n",
    "    ada1.train(x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
