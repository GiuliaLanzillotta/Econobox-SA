{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import pipeline as pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting replacement\n",
      "replacement done. \n",
      "\n",
      "Starting lemmatizing onreplaced_train_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Starting lemmatizing onreplaced_train_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemma = pipe.getTxtLemmatization([\"train_pos.txt\", \"train_neg.txt\"],\n",
    "                        stopwords = False,\n",
    "                        replace = True,\n",
    "                        replace_stanford=False,\n",
    "                        lemmatize = True,\n",
    "                        outputfiles = [\"redoing1_pos.txt\",\"redoing1_neg.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  redoing1_pos.txt\n",
      "Reading  redoing1_neg.txt\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import tokenizer as tok\n",
    "voc = tok.build_vocab(frequency_treshold=10,\n",
    "                file_name=\"redoing_vocab1.pkl\",\n",
    "                use_base_vocabulary=False,\n",
    "                base_vocabulary_name=\"stanford_vocab.pkl\",\n",
    "                input_files= [\"redoing1_pos.txt\", \"redoing1_neg.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on  redoing1_pos.txt\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "Working on  redoing1_neg.txt\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "summing duplicates (this can take a while)\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import cooc\n",
    "\n",
    "cooc_mat = cooc.build_cooc(\"redoing_vocab1.pkl\",\n",
    "               window_size=None,\n",
    "               weighting=\"None\",\n",
    "               output_name=\"redoing_cooc1.pkl\",\n",
    "               input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In get glove embedding\n",
      "Loading hyperparameters\n",
      "Loading pre-trained Stanford embedding\n",
      "Lost words:  0\n",
      "Opening co-occurrence matrix\n",
      "Started GloVe training\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n"
     ]
    }
   ],
   "source": [
    "import embedding.pipeline as emb\n",
    "\n",
    "emb1 = emb.get_glove_embedding(vocabulary_file=\"redoing_vocab1.pkl\",\n",
    "                        cooc_file=\"redoing_cooc1.pkl\",\n",
    "                        load_from_file=False,\n",
    "                        file_name = \"redoing_emb1.npz\",\n",
    "                        load_Stanford=True,\n",
    "                        train=True,\n",
    "                        save=True,\n",
    "                        train_epochs=10,\n",
    "                        train_eta=1e-3)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import pipeline as pipe\n",
    "import pickle\n",
    "from embedding.embedding_base import EmbeddingBase\n",
    "import pandas as pd\n",
    "from embedding import sentence_embedding\n",
    "import numpy as np\n",
    "from classifier import Adaboost_classi as classy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redoing_emb1.npz\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_1\n",
      "Working on  redoing1_pos.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Working on  redoing1_neg.txt\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Number of lines read: 200000\n",
      "Saving  D:\\embedding_matrices\\redoing1_training_0\n"
     ]
    }
   ],
   "source": [
    "with open(\"redoing_vocab1.pkl\", \"rb\") as f:\n",
    "    voc = pickle.load(f)\n",
    "with open(\"redoing_cooc1.pkl\", \"rb\") as f:\n",
    "    cooc = pickle.load(f)\n",
    "    \n",
    "emb = EmbeddingBase(embedding_name = \"redoing_emb1.npz\",\n",
    "                 embedding_dimension = 200,\n",
    "                 vocabulary = voc,\n",
    "                 cooc = cooc,\n",
    "                 load = True)\n",
    "\n",
    "mat1 = pipe.build_training_matrix(label = True,\n",
    "                          embedding = emb,\n",
    "                          input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"],\n",
    "                          label_values=None,\n",
    "                          input_entries=200000,\n",
    "                          sentence_dimesion = 200,\n",
    "                          output_location = \"D:\\\\embedding_matrices\\\\redoing1_training_1\")\n",
    "\n",
    "mat2 = pipe.build_training_matrix(label = True,\n",
    "                          embedding = emb,\n",
    "                          input_files=[\"redoing1_pos.txt\",\"redoing1_neg.txt\"],\n",
    "                          label_values=None,\n",
    "                          input_entries=200000,\n",
    "                          aggregation_fun=sentence_embedding.no_embeddings,\n",
    "                          sentence_dimesion = 100,\n",
    "                          output_location = \"D:\\\\embedding_matrices\\\\redoing1_training_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model.\n",
      "Training model\n",
      "[0.676425 0.66025  0.65     0.6452   0.64475 ]\n",
      "Building model.\n",
      "Training model\n",
      "[0.744975 0.74595  0.734775 0.7372   0.732075]\n"
     ]
    }
   ],
   "source": [
    "ada1 = classy.Adaboost_classi(embedding_dimension=200)\n",
    "arr0 = np.load(\"D://embedding_matrices//redoing1_training_0.npz\")\n",
    "arr1 = np.load(\"D://embedding_matrices//redoing1_training_1.npz\")\n",
    "arr1 = arr1['arr_0']\n",
    "arr0 = arr0['arr_0']\n",
    "x = arr0[:,:-1]\n",
    "y = arr0[:,-1]\n",
    "ada1.build()\n",
    "ada1.train(x,y)\n",
    "x = arr1[:,:-1]\n",
    "y = arr1[:,-1]\n",
    "ada1.build()\n",
    "ada1.train(x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
