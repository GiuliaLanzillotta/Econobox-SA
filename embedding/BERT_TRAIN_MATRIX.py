from classifier.classifier_base import ClassifierBase
import tensorflow as tf
import tensorflow_hub as hub
from matplotlib import pyplot as plt
from classifier import models_store_path
import numpy as np
import os
import bert
FullTokenizer = bert.bert_tokenization.FullTokenizer
from classifier.base_NN import BaseNN
from tqdm import tqdm
from data import tweetDF_location
from preprocessing.tweetDF import load_tweetDF
from bert import BertModelLayer
from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights
from data import bert_ckpt_file_location
from data import bert_config_file_location
from data import bert_vocab_location
from preprocessing.tweetDF import load_tweetDF
#abs_path = os.path.abspath(os.path.dirname(__file__))
import math
from tensorflow.keras.models import Model
from embedding import bert_matrix_train_location2

class BERT_TM_PP():
    """Class for preprocessing the data in
    a BERT embedding layer acceptable format.
    Note that this is a different format as in the
    BERT neural network classifier"""

    def __init__(self, datalist, max_seq_length):
        self.max_seq_len = max_seq_length
        self.bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1", trainable=False)
        self.vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()
        self.do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()
        self.tokenizer = FullTokenizer(self.vocab_file, self.do_lower_case)

        self.stokenslist = list(map(self.tokenizer.tokenize,datalist))
        self.stokenlistTRUNC = list(map(self.truncate_seqlength, self.stokenslist))
        self.stokenssep = list(map(self.add_seps, self.stokenlistTRUNC))

        self.input_ids = list(map(self.get_ids, self.stokenssep))
        self.input_masks = list(map(self.get_masks, self.stokenssep))
        self.input_segments = list(map(self.get_segments, self.stokenssep))

    def truncate_seqlength(self, stokenslist):
        if (len(stokenslist) > self.max_seq_len):
            stokenslist = stokenslist[:self.max_seq_len -2]
        return(stokenslist)


    def add_seps(self, stoken):
        """adds seperators as required by BERT"""
        return(["[CLS]"] + stoken + ["[SEP]"])


    def get_masks(self, stokenssep):
        """Mask for padding"""
        if len(stokenssep) > self.max_seq_len:
            print(len(stokenssep))
            raise IndexError("Token length more than max seq length!")
        return [1] * len(self.stokenssep) + [0] * (self.max_seq_len - len(self.stokenssep))

    def get_segments(self, stokenssep):
        """Segments: 0 for the first sequence, 1 for the second"""
        if len(stokenssep) > self.max_seq_len:
            raise IndexError("Token length more than max seq length!")
        segments = []
        current_segment_id = 0
        for token in stokenssep:
            segments.append(current_segment_id)
            if token == "[SEP]":
                current_segment_id = 1
        return segments + [0] * (self.max_seq_len - len(stokenssep))

    def get_ids(self, stokenssep):
        """Token ids from Tokenizer vocab"""
        token_ids = self.tokenizer.convert_tokens_to_ids(stokenssep)
        input_ids = token_ids + [0] * (self.max_seq_len - len(token_ids))
        return input_ids


def get_BERT_TM(data, output_location, batch_size, max_seq_length=128):
    """
    returns a "training matrix" with the rows corresponding to
    the embedding of a tweet generated by BERT and the sentiment label
    of the tweet. This can be used in all classifiers that take a training
    matrix as input
    """

    ## FIRST DEFINE THE MODEL
    print("Building Bert model.")
    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_mask")
    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="segment_ids")
    bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1", trainable=False)
    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
    model = Model(inputs=[input_word_ids, input_mask, segment_ids],
                  outputs=[pooled_output, sequence_output])


    #BATCH SIZE = CHUNK SIZE
    out_dim = 769
    input_entries = data.shape[0]
    #need to initialize something we delete this row later again
    output = np.zeros((1, out_dim))

    chunk_size = batch_size
    for start in range(0, input_entries, chunk_size):
        print(start)
        data_subset = data.iloc[start:start + chunk_size]
        data_subset_list = data_subset['text'].to_list()
        data_pp_subset = BERT_TM_PP(data_subset_list, max_seq_length=128)
        pool_embs, all_embs = model.predict([np.array(data_pp_subset.input_ids),
                                             np.array(data_pp_subset.input_masks),
                                             np.array(data_pp_subset.input_segments)])

        data_matrix = np.append(pool_embs.tolist(), data_subset['sent'].values.reshape(-1,1), axis=1)
        output = np.vstack((output, data_matrix))


    output = output[0:-1,:]

    np.savez(output_location, output)
    return output



